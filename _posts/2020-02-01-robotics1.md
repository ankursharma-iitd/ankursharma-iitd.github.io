---
title: "Probabilistic Graphical Models on a Robotic agent"
date: 2020-02-01
tags: [machine learning, robotics, computer vision]
excerpt: "Machine Learning, Robotics, Computer Vision"
mathjax: "true"
---

This is done as part of the course assignment for COL-864 (Robotics Vision) under Prof. Rohan Paul. Assignment Statement is given [here](https://www.dropbox.com/s/ra497raqmd7ed8d/Homework-I.pdf?dl=0).

## State Space
Here, the state space refers to all the possible valid positions in the 2D Grid (5x5) where the agent can possibly be. Each position is a tuple of the form (i, j) representing the ith row and the jth column of the board. This is also made clear from the diagram above. We will use Xt to unique denote the position of the agent (i.e. the state) at the time 't'. The set of all possible states is denoted by the symbol S. Hence, the entire state space consists of M2 states where M is the grid size. Hence, in this part, there are 25 states.

S = {(i, j): 0 ‚â§ i ‚â§ 4, 0 ‚â§ j ‚â§ 4; i, j ùûä N}

## Observation Space
Since the position of the agent is not visible from the surface of the lake, we use sensors that emit two sounds coming from the Rotor and Bump. Each sound detected by the sensor is a boolean variable indicating the presence or absence of the sound. Hence, the entire observation space is a tuple of 2 boolean variables Bt and Rt. Hence, it consists of four different observations at each time step. The set O completely describes the observation space. We will assume that the state sequence starts at t = 0; for various uninteresting reasons, we will assume that evidence starts arriving at t = 1 rather than t = 0. 

B = {"bump", "no bump"}
R = {"rotor", "no rotor"}
O = {(Bi, Rj): Bi ùûä {"bump", "no bump"},  Rj ùûä {"rotor", "no rotor"}}

We will use Bt and Rt to denote the observation taken from the sensor at some time 't'.

## Action Space
It refers to the space of all possible actions taken at each state. As given in the question, the agent is likely to take all the four possible sections {UP, DOWN, LEFT, RIGHT} with an equal probability. If the agent is at a corner position, some of the actions won't be feasible in that state, but the agent won't know about it before execution. As a result, the agent won't change its state, but the action will still be sampled from the same uniform distribution. At will denote the action taken by the agent at a time 't'.

A = {UP, DOWN, LEFT, RIGHT} 

## Transition Model
It refers to modeling the distribution of P(Xt|Xt-1) where Xt denotes the state of the agent at the time 't'. Fig below denotes all possible transitions and completely describes the transition model.

{% include image.html url="/images/robotics1/1.png" description="All possible transitions within the state space and the spatial likelihood of observations" %}

## Observation Model
The probabilities P(Rt | Xt) and P(Bt | Xt) have been shown below respectively. Dark regions denote 0.1 probability while the light regions denote 0.9 probability.

{% include image.html url="/images/robotics1/2.png" description="All possible transitions within the state space and the spatial likelihood of observations" %}

## Probabilistic Graphical Model (PGM)

{% include image.html url="/images/robotics1/3.png" description="Bayesian Network showing the causal cause-effect relationships" %}

## Conditional Independence Assumptions
{% include image.html url="/images/robotics1/4.png" description="" %}

## Joint Distribution
{% include image.html url="/images/robotics1/5.png" description="" %}

## Inference Task
{% include image.html url="/images/robotics1/6.png" description="" %}

### Simulating the agent for 10 timesteps
{% include image.html url="/images/robotics1/7.png" description="" %}

### Ground Locations
{% include image.html url="/images/robotics1/8.png" description="" %}

## Filtering Task
{% include image.html url="/images/robotics1/9.png" description="Log-Likelihood of the normalized probabilities estimated for each state by filtering task" %}

We see how the maximum log-likelihood gives a good estimation of the position of the agent at each time step. We see that there are some mispredictions initially but as soon as we are able to collect more and more evidence and observations from the rotor and bump, we are able to improve the estimation.

{% include image.html url="/images/robotics1/10.png" description="Estimated v/s Actual Location at each timestep (Both the Observations)" %}

{% include image.html url="/images/robotics1/11.gif" description="Dynamic visualization of the movement of the agent
" %}

As we see from Fig above, we are able to accurately predict using 6 out of 10 times. We will later see how this result compares using only a single observation and using backward smoothing.

## Filtering using only Single Modality

We repeat the same experiment as above but only with a single modality this time. We keep only bump observation ("bump" or "no bump") as a signal for forwarding filtering.

{% include image.html url="/images/robotics1/12.png" description="Log-Likelihood of the normalized probabilities estimated for each state (ONLY bump)" %}

{% include image.html url="/images/robotics1/13.png" description="Estimated v/s Actual Location at each timestep (ONLY bump observations)" %}

{% include image.html url="/images/robotics1/14.gif" description="Dynamic visualization of the movement of the agent (ONLY bump observations)" %}

We see that using only bump observations, we are getting more mispredictions since we are using less evidence available. Now, we are able to see only 4 cases (out of 10) when we are accurately able to predict the agent's position.

## Prediction Task
This is the task of computing the posterior distribution over the future state, given all evidence to date. 

{% include image.html url="/images/robotics1/15.png" description="Prediction log-likelihood probabilities at the next time step (t = 10) using all evidence" %}

{% include image.html url="/images/robotics1/16.png" description="Prediction log-likelihood probabilities at the next 5 time steps using all evidence" %}

We see that the range of the colormap reduces as the time steps increases. This means that as we try to predict more and more about the future states using the current evidence, we are losing our confidence in the predictions, leading to a decrease in the probability bandwidth. This leads to uncertainty in the state predictions.

{% include image.html url="/images/robotics1/17.png" description="Prediction state estimates at all the time steps (0 to 14)" %}

We see an increase in the uncertainty in the movement of agent at the future time steps. The agent merely oscillates around the location at the 10th time step and this uncertainty reaches its peak at the 15th time step.

## Smoothing Task

{% include image.html url="/images/robotics1/18.png" description=" Estimated v/s Actual Location at each timestep (Smoothing Task)" %}

We see from Fig above that the backward algorithm actually helps in fixing the prediction at t = 0 by making use of the future evidence. Now, we see that there are 7 cases out of 10 that are accurately able to estimate the correct position.  Fig below shows the dynamic gif representing the movement of the agent and the estimated prediction from the maximum log-likelihood predictions. The improvement in predictions is clearly realized as expected from the Backward Algorithm.

{% include image.html url="/images/robotics1/19.gif" description=" Dynamic Estimated v/s Actual Location at each timestep (Smoothing Task)" %}

## Most Likely Explanation (Viterbi Algorithm)

{% include image.html url="/images/robotics1/20.png" description=" Dynamic Estimated v/s Actual Location at each timestep (Smoothing Task)" %}



